{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Basic packages\nimport os\nimport re\nimport random\nimport time\nimport gc\nfrom collections import Counter, defaultdict\nimport heapq\n\n# Torch & DL utilities\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.nn.utils.rnn import pad_sequence\n\n# NLP utilities\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom datasets import load_dataset\n\n# Progress bar\nfrom tqdm import tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:21:34.122607Z","iopub.execute_input":"2025-05-15T18:21:34.122869Z","iopub.status.idle":"2025-05-15T18:21:41.501183Z","shell.execute_reply.started":"2025-05-15T18:21:34.122849Z","shell.execute_reply":"2025-05-15T18:21:41.500622Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"opus100\",\"en-it\")\n\ntrain_data = dataset[\"train\"].shuffle(seed=42).select(range(500_000))\nval_data = dataset[\"validation\"]\ntest_data = dataset[\"test\"]\n\nprint(\"Train size:\", len(train_data))\nprint(\"Validation size:\", len(val_data))\nprint(\"Test size:\", len(test_data))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:21:41.502448Z","iopub.execute_input":"2025-05-15T18:21:41.503132Z","iopub.status.idle":"2025-05-15T18:21:53.871491Z","shell.execute_reply.started":"2025-05-15T18:21:41.503109Z","shell.execute_reply":"2025-05-15T18:21:53.870833Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/65.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89f4c9e174c546bcb29a6af1501f19d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/223k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86f50f8c85a54114b46ed269718225ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/91.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2318cc288fe4ce6980e93872106ee85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/220k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a9469d28f914d3db212d6cfc94865b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"534611b6babc4c64b05ab59116353cb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9762bd35c3e1489db74aa14629884468"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98a4a2801eb8419e98ccc39659be6e62"}},"metadata":{}},{"name":"stdout","text":"Train size: 500000\nValidation size: 2000\nTest size: 2000\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def clean_text(text):\n    text = re.sub(r\"[^a-zA-ZàèéìòùÀÈÉÌÒÙ\\s]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n\ndef apply_cleaning(dataset_split):\n    return dataset_split.map(lambda x: {\n        \"translation\": {\n            \"en\": clean_text(x[\"translation\"][\"en\"]),\n            \"it\": clean_text(x[\"translation\"][\"it\"]),\n        }\n    })\n\ntrain_data = apply_cleaning(train_data)\nval_data = apply_cleaning(val_data)\ntest_data = apply_cleaning(test_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:21:53.872150Z","iopub.execute_input":"2025-05-15T18:21:53.872337Z","iopub.status.idle":"2025-05-15T18:22:45.140426Z","shell.execute_reply.started":"2025-05-15T18:21:53.872321Z","shell.execute_reply":"2025-05-15T18:22:45.139616Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2294d1950c04ce09fff650dbe37bc3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"761765c182684fb28c99eb4c4077c264"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df5cfb8031784d8d9065a16a89de317e"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def clean_and_tokenize(example):\n    # Lowercase and whitespace-tokenize both source and target\n    src = example[\"translation\"][\"en\"].lower().strip().split()\n    tgt = example[\"translation\"][\"it\"].lower().strip().split()\n    return {\"src_tokens\": src, \"tgt_tokens\": tgt}\n\n# Apply to all splits\ntrain_data = train_data.map(clean_and_tokenize)\nval_data = val_data.map(clean_and_tokenize)\ntest_data = test_data.map(clean_and_tokenize)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:22:45.141753Z","iopub.execute_input":"2025-05-15T18:22:45.141974Z","iopub.status.idle":"2025-05-15T18:23:09.393305Z","shell.execute_reply.started":"2025-05-15T18:22:45.141958Z","shell.execute_reply":"2025-05-15T18:23:09.392756Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f627df5051b4467a59918a1c3516218"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d969e1063a3476abd88282c9476d846"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c248ba573047118c15e7db40a352ec"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def remove_long_sentences(example, max_len=10):\n    return len(example[\"src_tokens\"]) <= max_len and len(example[\"tgt_tokens\"]) <= max_len\ntrain_data = train_data.filter(remove_long_sentences)\nval_data = val_data.filter(remove_long_sentences)\ntest_data = test_data.filter(remove_long_sentences)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:23:09.393955Z","iopub.execute_input":"2025-05-15T18:23:09.394125Z","iopub.status.idle":"2025-05-15T18:23:23.113607Z","shell.execute_reply.started":"2025-05-15T18:23:09.394110Z","shell.execute_reply":"2025-05-15T18:23:23.112783Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/500000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d27e4c2c65e4485996cc1e3beb3f33dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5d055e18ed3482ca968002ec573854f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0de45ae944bf4420a0b3ea6a729cdb7b"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def build_vocab(token_lists, max_vocab_size=12000):\n    counter = Counter()\n    for tokens in token_lists:\n        counter.update(tokens)\n    most_common = counter.most_common(max_vocab_size - 4)\n\n    vocab = {\n        \"<PAD>\": 0,\n        \"<SOS>\": 1,\n        \"<EOS>\": 2,\n        \"<UNK>\": 3\n    }\n\n    for idx, (token, _) in enumerate(most_common):\n        vocab[token] = idx + 4\n\n    return vocab\n\n# Build English (source) and Italian (target) vocabularies\nen_vocab = build_vocab(train_data[\"src_tokens\"], max_vocab_size=10000)\nit_vocab = build_vocab(train_data[\"tgt_tokens\"], max_vocab_size=10000)\nen_itos = {i: s for s, i in en_vocab.items()}\nit_itos = {i: s for s, i in it_vocab.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:23:23.114441Z","iopub.execute_input":"2025-05-15T18:23:23.114651Z","iopub.status.idle":"2025-05-15T18:23:32.307778Z","shell.execute_reply.started":"2025-05-15T18:23:23.114634Z","shell.execute_reply":"2025-05-15T18:23:32.306949Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Count token frequencies\nsrc_counter = Counter(tok for sent in train_data[\"src_tokens\"] for tok in sent)\ntgt_counter = Counter(tok for sent in train_data[\"tgt_tokens\"] for tok in sent)\n\n# Replace infrequent tokens with <UNK>\ndef mark_rare_tokens(tokens, counter, threshold):\n    return [tok if counter[tok] >= threshold else \"<UNK>\" for tok in tokens]\n\ndef replace_with_unk(example):\n    src = mark_rare_tokens(example[\"src_tokens\"], src_counter, threshold=2)\n    tgt = mark_rare_tokens(example[\"tgt_tokens\"], tgt_counter, threshold=2)\n    return {\"src_tokens\": src, \"tgt_tokens\": tgt}\n\n# Apply UNK replacement\ntrain_data = train_data.map(replace_with_unk)\nval_data = val_data.map(replace_with_unk)\ntest_data = test_data.map(replace_with_unk)\n\n# Remove pairs containing <UNK>\ndef remove_unk_pairs(example):\n    return \"<UNK>\" not in example[\"src_tokens\"] and \"<UNK>\" not in example[\"tgt_tokens\"]\n\ntrain_data = train_data.filter(remove_unk_pairs)\nval_data = val_data.filter(remove_unk_pairs)\ntest_data = test_data.filter(remove_unk_pairs)\n\n# Show resulting dataset sizes\nprint(f\"Filtered Train Size: {len(train_data):,}\")\nprint(f\"Filtered Validation Size: {len(val_data):,}\")\nprint(f\"Filtered Test Size: {len(test_data):,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:23:32.308644Z","iopub.execute_input":"2025-05-15T18:23:32.308902Z","iopub.status.idle":"2025-05-15T18:24:28.698161Z","shell.execute_reply.started":"2025-05-15T18:23:32.308879Z","shell.execute_reply":"2025-05-15T18:24:28.697397Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/348175 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2a0799c3fe6454abb8e560537eb9c3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1226 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"569038c5e10546c1aca2c53077708da5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1206 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8270bdf3a54a4982a77bf30197deb1cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/348175 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13a8c146b6d74f699b39cdef1b6d8149"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1226 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f55122fbaf6458d910932321a39c25c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1206 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5552754f9132439ea3f4e15ade6db6f4"}},"metadata":{}},{"name":"stdout","text":"Filtered Train Size: 297,057\nFiltered Validation Size: 883\nFiltered Test Size: 863\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(train_data[12])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:24:28.699009Z","iopub.execute_input":"2025-05-15T18:24:28.699232Z","iopub.status.idle":"2025-05-15T18:24:28.703586Z","shell.execute_reply.started":"2025-05-15T18:24:28.699214Z","shell.execute_reply":"2025-05-15T18:24:28.702915Z"}},"outputs":[{"name":"stdout","text":"{'translation': {'en': 'Dont ever say that again', 'it': 'Non dirlo mai piu'}, 'src_tokens': ['dont', 'ever', 'say', 'that', 'again'], 'tgt_tokens': ['non', 'dirlo', 'mai', 'piu']}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    def __init__(self, src_tokens_list, tgt_tokens_list, src_vocab, tgt_vocab, max_len=10):\n        self.src_tokens = src_tokens_list\n        self.tgt_tokens = tgt_tokens_list\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.src_tokens)\n\n    def __getitem__(self, idx):\n        src_seq = self.src_tokens[idx]\n        tgt_seq = self.tgt_tokens[idx]\n\n        src_ids = [self.src_vocab.get(token, self.src_vocab[\"<UNK>\"]) for token in src_seq]\n        tgt_ids = [self.tgt_vocab[\"<SOS>\"]] + \\\n                  [self.tgt_vocab.get(token, self.tgt_vocab[\"<UNK>\"]) for token in tgt_seq] + \\\n                  [self.tgt_vocab[\"<EOS>\"]]\n\n        # Pad or truncate\n        src_ids = self._pad_or_truncate(src_ids, self.src_vocab[\"<PAD>\"])\n        tgt_ids = self._pad_or_truncate(tgt_ids, self.tgt_vocab[\"<PAD>\"])\n\n        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n\n    def _pad_or_truncate(self, ids, pad_idx):\n        if len(ids) > self.max_len:\n            return ids[:self.max_len]\n        else:\n            return ids + [pad_idx] * (self.max_len - len(ids))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:24:28.704468Z","iopub.execute_input":"2025-05-15T18:24:28.704674Z","iopub.status.idle":"2025-05-15T18:24:28.727284Z","shell.execute_reply.started":"2025-05-15T18:24:28.704658Z","shell.execute_reply":"2025-05-15T18:24:28.726704Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Create dataset instances\ntrain_dataset = TranslationDataset(\n    src_tokens_list=train_data[\"src_tokens\"],\n    tgt_tokens_list=train_data[\"tgt_tokens\"],\n    src_vocab=en_vocab,\n    tgt_vocab=it_vocab,\n    max_len=10\n)\n\nval_dataset = TranslationDataset(\n    src_tokens_list=val_data[\"src_tokens\"],\n    tgt_tokens_list=val_data[\"tgt_tokens\"],\n    src_vocab=en_vocab,\n    tgt_vocab=it_vocab,\n    max_len=10\n)\n\ntest_dataset = TranslationDataset(\n    src_tokens_list=test_data[\"src_tokens\"],\n    tgt_tokens_list=test_data[\"tgt_tokens\"],\n    src_vocab=en_vocab,\n    tgt_vocab=it_vocab,\n    max_len=10\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:24:28.729300Z","iopub.execute_input":"2025-05-15T18:24:28.729587Z","iopub.status.idle":"2025-05-15T18:24:35.767931Z","shell.execute_reply.started":"2025-05-15T18:24:28.729563Z","shell.execute_reply":"2025-05-15T18:24:35.767154Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def get_dataloader(dataset, batch_size=64, shuffle=True):\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        num_workers=4,\n        pin_memory=True\n    )\n\nbatch_size = 128\n\ntrain_loader = get_dataloader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = get_dataloader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = get_dataloader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:24:35.768770Z","iopub.execute_input":"2025-05-15T18:24:35.768976Z","iopub.status.idle":"2025-05-15T18:24:35.774008Z","shell.execute_reply.started":"2025-05-15T18:24:35.768959Z","shell.execute_reply":"2025-05-15T18:24:35.773261Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class Seq2SeqAttention(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_dim,\n                 src_pad_idx, tgt_sos_idx, tgt_eos_idx, n_layers=1, dropout=0.2):\n        super().__init__()\n\n        # Embedding layers\n        self.encoder_embedding = nn.Embedding(src_vocab_size, embedding_dim, padding_idx=src_pad_idx)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, embedding_dim, padding_idx=src_pad_idx)\n\n        # Encoder: bidirectional LSTM\n        self.encoder_lstm = nn.LSTM(\n            input_size=embedding_dim,\n            hidden_size=hidden_dim,\n            num_layers=n_layers,\n            dropout=dropout if n_layers > 1 else 0,\n            batch_first=True,\n            bidirectional=True\n        )\n\n        # Decoder: unidirectional LSTM\n        self.decoder_lstm = nn.LSTM(\n            input_size=embedding_dim + hidden_dim * 2,\n            hidden_size=hidden_dim,\n            num_layers=n_layers,\n            dropout=dropout if n_layers > 1 else 0,\n            batch_first=True\n        )\n\n        # Attention: project decoder hidden → same size as encoder outputs\n        self.attn = nn.Linear(hidden_dim, hidden_dim * 2)\n\n        # Final output layer\n        self.fc_out = nn.Linear(hidden_dim + hidden_dim * 2, tgt_vocab_size)\n\n        # Project encoder hidden/cell → decoder hidden/cell\n        self.bridge = nn.Linear(hidden_dim * 2, hidden_dim)\n\n        self.dropout = nn.Dropout(dropout)\n        self.hidden_dim = hidden_dim\n        self.tgt_sos_idx = tgt_sos_idx\n        self.tgt_eos_idx = tgt_eos_idx\n        self.n_layers = n_layers\n\n    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n        batch_size, tgt_len = tgt.size()\n        vocab_size = self.fc_out.out_features\n\n        # ---- Encoder ----\n        embedded_src = self.dropout(self.encoder_embedding(src))  # [B, src_len, emb]\n        encoder_outputs, (hidden, cell) = self.encoder_lstm(embedded_src)  # encoder_outputs: [B, src_len, 2H]\n\n        hidden = self._combine_directions(hidden)  # [n_layers, B, 2H]\n        cell = self._combine_directions(cell)\n\n        hidden = torch.tanh(self.bridge(hidden))  # [n_layers, B, H]\n        cell = torch.tanh(self.bridge(cell))      # [n_layers, B, H]\n\n        # ---- Decoder ----\n        inputs = tgt[:, 0]  # <SOS>\n        outputs = torch.zeros(batch_size, tgt_len - 1, vocab_size).to(src.device)\n\n        for t in range(1, tgt_len):\n            input_emb = self.dropout(self.decoder_embedding(inputs)).unsqueeze(1)  # [B, 1, emb]\n\n            # Attention\n            attn_query = self.attn(hidden[-1]).unsqueeze(1)  # [B, 1, 2H]\n            attn_weights = torch.bmm(attn_query, encoder_outputs.transpose(1, 2))  # [B, 1, src_len]\n            attn_weights = F.softmax(attn_weights, dim=2)\n            context = torch.bmm(attn_weights, encoder_outputs)  # [B, 1, 2H]\n\n            # Decoder step\n            rnn_input = torch.cat((input_emb, context), dim=2)  # [B, 1, emb + 2H]\n            output, (hidden, cell) = self.decoder_lstm(rnn_input, (hidden, cell))  # output: [B, 1, H]\n\n            pred = self.fc_out(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))  # [B, vocab]\n            outputs[:, t - 1] = pred\n\n            # Next input\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = pred.argmax(1)\n            inputs = tgt[:, t] if teacher_force else top1\n\n        return outputs  # [B, tgt_len - 1, vocab]\n\n    def predict(self, src, max_len=50, device='cpu'):\n        self.eval()\n        src = src.unsqueeze(0).to(device)  # [1, src_len]\n\n        with torch.no_grad():\n            embedded_src = self.encoder_embedding(src)  # [1, src_len, emb]\n            encoder_outputs, (hidden, cell) = self.encoder_lstm(embedded_src)\n\n            hidden = self._combine_directions(hidden)  # [n_layers, 1, 2H]\n            cell = self._combine_directions(cell)\n\n            hidden = torch.tanh(self.bridge(hidden))  # [n_layers, 1, H]\n            cell = torch.tanh(self.bridge(cell))      # [n_layers, 1, H]\n\n            inputs = torch.tensor([self.tgt_sos_idx], device=device)  # [1]\n            outputs = []\n\n            for _ in range(max_len):\n                input_emb = self.decoder_embedding(inputs).unsqueeze(1)  # [1, 1, emb]\n\n                attn_query = self.attn(hidden[-1]).unsqueeze(1)  # [1, 1, 2H]\n                attn_weights = torch.bmm(attn_query, encoder_outputs.transpose(1, 2))  # [1, 1, src_len]\n                attn_weights = F.softmax(attn_weights, dim=2)\n                context = torch.bmm(attn_weights, encoder_outputs)  # [1, 1, 2H]\n\n                rnn_input = torch.cat((input_emb, context), dim=2)  # [1, 1, emb + 2H]\n                output, (hidden, cell) = self.decoder_lstm(rnn_input, (hidden, cell))  # output: [1, 1, H]\n\n                pred = self.fc_out(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))  # [1, vocab]\n                top1 = pred.argmax(1).item()\n\n                if top1 == self.tgt_eos_idx:\n                    break\n                outputs.append(top1)\n                inputs = torch.tensor([top1], device=device)\n\n        return outputs  # List[int]\n\n    def _combine_directions(self, states):\n        # [2*n_layers, B, H] → [n_layers, B, 2H]\n        return torch.cat((states[0::2], states[1::2]), dim=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:24:35.774788Z","iopub.execute_input":"2025-05-15T18:24:35.775248Z","iopub.status.idle":"2025-05-15T18:24:35.797574Z","shell.execute_reply.started":"2025-05-15T18:24:35.775229Z","shell.execute_reply":"2025-05-15T18:24:35.796867Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = Seq2SeqAttention(\n    src_vocab_size=len(en_vocab),\n    tgt_vocab_size=len(it_vocab),\n    embedding_dim=256,\n    hidden_dim=512,\n    src_pad_idx=en_vocab[\"<PAD>\"],\n    tgt_sos_idx=it_vocab[\"<SOS>\"],\n    tgt_eos_idx=it_vocab[\"<EOS>\"],\n    n_layers=2,\n    dropout=0.3\n).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:24:35.798288Z","iopub.execute_input":"2025-05-15T18:24:35.798767Z","iopub.status.idle":"2025-05-15T18:24:36.437038Z","shell.execute_reply.started":"2025-05-15T18:24:35.798741Z","shell.execute_reply":"2025-05-15T18:24:36.436499Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Padding index to be ignored in loss calculation\npad_idx = it_vocab[\"<PAD>\"]\n\n# Loss function: ignores padded positions\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\n# Optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Aliases for vocab access during translation/decoding\nsrc_vocab = en_vocab\ntgt_vocab = it_vocab\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:24:36.437699Z","iopub.execute_input":"2025-05-15T18:24:36.437942Z","iopub.status.idle":"2025-05-15T18:24:40.614166Z","shell.execute_reply.started":"2025-05-15T18:24:36.437920Z","shell.execute_reply":"2025-05-15T18:24:40.613648Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, optimizer, criterion, device, clip=1.0, teacher_forcing_ratio=0.5):\n    model.train()\n    epoch_loss = 0\n\n    for src, tgt in tqdm(dataloader, desc=\"Training\"):\n        src, tgt = src.to(device), tgt.to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass with teacher forcing\n        output = model(src, tgt, teacher_forcing_ratio)  # [B, tgt_len-1, vocab]\n\n        # Align shapes for loss computation\n        output = output.contiguous().view(-1, output.shape[-1])  # [B*(tgt_len-1), vocab]\n        tgt = tgt[:, 1:].contiguous().view(-1)                  # [B*(tgt_len-1)]\n\n        loss = criterion(output, tgt)\n        loss.backward()\n\n        # Gradient clipping to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    return epoch_loss / len(dataloader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:24:40.615131Z","iopub.execute_input":"2025-05-15T18:24:40.615590Z","iopub.status.idle":"2025-05-15T18:24:40.621031Z","shell.execute_reply.started":"2025-05-15T18:24:40.615561Z","shell.execute_reply":"2025-05-15T18:24:40.620152Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def evaluate_one_epoch(model, dataloader, criterion, device):\n    model.eval()\n    epoch_loss = 0\n\n    with torch.no_grad():\n        for src, tgt in tqdm(dataloader, desc=\"Validation\"):\n            src, tgt = src.to(device), tgt.to(device)\n\n            # No teacher forcing during evaluation\n            output = model(src, tgt, teacher_forcing_ratio=0.0)  # [B, tgt_len-1, vocab]\n\n            # Align for loss computation\n            output = output.contiguous().view(-1, output.shape[-1])  # [B*(tgt_len-1), vocab]\n            tgt = tgt[:, 1:].contiguous().view(-1)                  # [B*(tgt_len-1)]\n\n            loss = criterion(output, tgt)\n            epoch_loss += loss.item()\n\n    return epoch_loss / len(dataloader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:24:40.621988Z","iopub.execute_input":"2025-05-15T18:24:40.622241Z","iopub.status.idle":"2025-05-15T18:24:40.676953Z","shell.execute_reply.started":"2025-05-15T18:24:40.622219Z","shell.execute_reply":"2025-05-15T18:24:40.676284Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def train_loop(model, train_loader, val_loader, optimizer, criterion, device,\n               n_epochs=10, clip=1.0, teacher_forcing_ratio=0.5):\n    \n    best_val_loss = float('inf')\n\n    for epoch in range(n_epochs):\n        start_time = time.time()\n\n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device,\n                                     clip=clip, teacher_forcing_ratio=teacher_forcing_ratio)\n        val_loss = evaluate_one_epoch(model, val_loader, criterion, device)\n\n        end_time = time.time()\n        mins, secs = divmod(int(end_time - start_time), 60)\n\n        print(f\"\\nEpoch {epoch + 1:02} | Time: {mins}m {secs}s\")\n        print(f\"  Train Loss: {train_loss:.4f}\")\n        print(f\"  Val   Loss: {val_loss:.4f}\")\n\n        # Save the model if it has improved\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), \"best_model.pt\")\n            print(\"Saved best model.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:24:40.677759Z","iopub.execute_input":"2025-05-15T18:24:40.678007Z","iopub.status.idle":"2025-05-15T18:24:40.697473Z","shell.execute_reply.started":"2025-05-15T18:24:40.677984Z","shell.execute_reply":"2025-05-15T18:24:40.696764Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"train_loop(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    optimizer=optimizer,\n    criterion=criterion,\n    n_epochs=10,\n    clip=1.0,\n    teacher_forcing_ratio=0.4,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:24:40.698208Z","iopub.execute_input":"2025-05-15T18:24:40.698498Z","iopub.status.idle":"2025-05-15T19:06:50.523530Z","shell.execute_reply.started":"2025-05-15T18:24:40.698479Z","shell.execute_reply":"2025-05-15T19:06:50.522410Z"}},"outputs":[{"name":"stderr","text":"Training: 100%|██████████| 2321/2321 [04:14<00:00,  9.12it/s]\nValidation: 100%|██████████| 7/7 [00:00<00:00, 11.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 01 | Time: 4m 15s\n  Train Loss: 4.9452\n  Val   Loss: 4.6843\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2321/2321 [04:12<00:00,  9.18it/s]\nValidation: 100%|██████████| 7/7 [00:00<00:00, 10.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 02 | Time: 4m 13s\n  Train Loss: 4.1295\n  Val   Loss: 4.3157\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2321/2321 [04:11<00:00,  9.24it/s]\nValidation: 100%|██████████| 7/7 [00:00<00:00, 12.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 03 | Time: 4m 11s\n  Train Loss: 3.7791\n  Val   Loss: 4.1158\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2321/2321 [04:10<00:00,  9.25it/s]\nValidation: 100%|██████████| 7/7 [00:00<00:00, 11.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 04 | Time: 4m 11s\n  Train Loss: 3.5443\n  Val   Loss: 3.9616\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2321/2321 [04:11<00:00,  9.21it/s]\nValidation: 100%|██████████| 7/7 [00:00<00:00, 11.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 05 | Time: 4m 12s\n  Train Loss: 3.3719\n  Val   Loss: 3.9011\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2321/2321 [04:11<00:00,  9.21it/s]\nValidation: 100%|██████████| 7/7 [00:00<00:00, 11.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 06 | Time: 4m 12s\n  Train Loss: 3.2350\n  Val   Loss: 3.8525\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2321/2321 [04:11<00:00,  9.24it/s]\nValidation: 100%|██████████| 7/7 [00:00<00:00, 11.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 07 | Time: 4m 11s\n  Train Loss: 3.1256\n  Val   Loss: 3.7992\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2321/2321 [04:11<00:00,  9.23it/s]\nValidation: 100%|██████████| 7/7 [00:00<00:00, 11.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 08 | Time: 4m 12s\n  Train Loss: 3.0350\n  Val   Loss: 3.7498\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2321/2321 [04:11<00:00,  9.22it/s]\nValidation: 100%|██████████| 7/7 [00:00<00:00, 11.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 09 | Time: 4m 12s\n  Train Loss: 2.9486\n  Val   Loss: 3.7433\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2321/2321 [04:11<00:00,  9.22it/s]\nValidation: 100%|██████████| 7/7 [00:00<00:00, 11.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10 | Time: 4m 12s\n  Train Loss: 2.8781\n  Val   Loss: 3.7002\nSaved best model.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\ndef compute_bleu(model, dataloader, tgt_vocab, idx_to_tgt, device, max_len=50, num_samples=100):\n    model.eval()\n    references = []\n    hypotheses = []\n\n    sos_idx = tgt_vocab[\"<SOS>\"]\n    eos_idx = tgt_vocab[\"<EOS>\"]\n    pad_idx = tgt_vocab[\"<PAD>\"]\n\n    with torch.no_grad():\n        for i, (src_batch, tgt_batch) in enumerate(dataloader):\n            if i * src_batch.size(0) > num_samples:\n                break\n\n            src_batch = src_batch.to(device)\n            tgt_batch = tgt_batch.to(device)\n\n            for src, tgt in zip(src_batch, tgt_batch):\n                # Inference\n                pred_ids = model.predict(src, max_len=max_len, device=device)\n\n                # Reference (target) - skip <SOS>, <EOS>, <PAD>\n                ref = [idx_to_tgt.get(idx.item(), \"<UNK>\") \n                       for idx in tgt if idx.item() not in {pad_idx, sos_idx, eos_idx}]\n\n                # Hypothesis (prediction) - skip <EOS>\n                hyp = [idx_to_tgt.get(idx, \"<UNK>\") \n                       for idx in pred_ids if idx != eos_idx]\n\n                references.append([ref])  # reference must be a list of lists\n                hypotheses.append(hyp)\n\n    bleu = corpus_bleu(references, hypotheses)\n    return bleu  # optionally return bleu * 100\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T19:07:45.742651Z","iopub.execute_input":"2025-05-15T19:07:45.743202Z","iopub.status.idle":"2025-05-15T19:07:45.750104Z","shell.execute_reply.started":"2025-05-15T19:07:45.743172Z","shell.execute_reply":"2025-05-15T19:07:45.749506Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def translate_and_print(model, sentence, src_vocab, tgt_vocab, idx_to_tgt, device, max_len=50):\n    model.eval()\n\n    # Preprocess and tokenize\n    tokens = sentence.lower().strip().split()\n    src_ids = [src_vocab.get(tok, src_vocab[\"<UNK>\"]) for tok in tokens]\n\n    # Convert to tensor and run prediction\n    src_tensor = torch.tensor(src_ids, dtype=torch.long).to(device)\n    pred_ids = model.predict(src_tensor, max_len=max_len, device=device)\n\n    # Decode predicted token ids\n    pred_tokens = [idx_to_tgt.get(idx, \"<UNK>\") for idx in pred_ids]\n\n    print(\"Source:      \", sentence)\n    print(\"Translation: \", \" \".join(pred_tokens))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T19:07:49.014175Z","iopub.execute_input":"2025-05-15T19:07:49.014717Z","iopub.status.idle":"2025-05-15T19:07:49.019442Z","shell.execute_reply.started":"2025-05-15T19:07:49.014695Z","shell.execute_reply":"2025-05-15T19:07:49.018844Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Inverse vocab\nidx_to_tgt = {idx: tok for tok, idx in it_vocab.items()}\n\n# BLEU score on validation set (first 100 samples)\nbleu_score = compute_bleu(model, test_loader, it_vocab, idx_to_tgt, device, num_samples=500)\nprint(f\"\\nBLEU Score (100 samples): {bleu_score:.4f}\")\n\n# Example translation\nexample_sentence = \"how are you\"\ntranslate_and_print(model, example_sentence, en_vocab, it_vocab, idx_to_tgt, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T19:07:50.358371Z","iopub.execute_input":"2025-05-15T19:07:50.359137Z","iopub.status.idle":"2025-05-15T19:07:53.353040Z","shell.execute_reply.started":"2025-05-15T19:07:50.359108Z","shell.execute_reply":"2025-05-15T19:07:53.352128Z"}},"outputs":[{"name":"stdout","text":"\nBLEU Score (100 samples): 0.1146\nSource:       how are you\nTranslation:  come sei come come\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"sentences = [\n    \"good morning\",\n    \"how are you\",\n    \"do you have money\",\n    \"I am very happy\",\n    \"nice to meet you \",\n    \"the weather is nice\",\n    \"i have been waiting for you\"\n]\nfor sentence in sentences:\n    translate_and_print(model,sentence,en_vocab,it_vocab,idx_to_tgt,device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T19:09:55.854404Z","iopub.execute_input":"2025-05-15T19:09:55.854691Z","iopub.status.idle":"2025-05-15T19:09:55.891596Z","shell.execute_reply.started":"2025-05-15T19:09:55.854670Z","shell.execute_reply":"2025-05-15T19:09:55.891026Z"}},"outputs":[{"name":"stdout","text":"Source:       good morning\nTranslation:  buongiorno\nSource:       how are you\nTranslation:  come sei come come\nSource:       do you have money\nTranslation:  hai dei soldi\nSource:       I am very happy\nTranslation:  sono molto felice felice\nSource:       nice to meet you \nTranslation:  piacere di conoscerti\nSource:       the weather is nice\nTranslation:  la <UNK> è è un\nSource:       i have been waiting for you\nTranslation:  ho aspettato aspettando per te\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"\nsrc_batch, tgt_batch = next(iter(train_loader))\noutput = model(src_batch.to(device), tgt_batch.to(device), teacher_forcing_ratio=0.0)\n\npredictions = output.argmax(dim=-1)  # shape: [B, T]\nunk_id = tgt_vocab[\"<UNK>\"]\nunk_ratio = (predictions == unk_id).float().mean().item()\n\nprint(f\"Ratio of predicted <UNK>: {unk_ratio:.2%}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T19:07:58.540578Z","iopub.execute_input":"2025-05-15T19:07:58.540861Z","iopub.status.idle":"2025-05-15T19:07:58.866561Z","shell.execute_reply.started":"2025-05-15T19:07:58.540841Z","shell.execute_reply":"2025-05-15T19:07:58.865651Z"}},"outputs":[{"name":"stdout","text":"Ratio of predicted <UNK>: 2.34%\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}