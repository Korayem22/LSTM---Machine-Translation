{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-15T18:21:34.122869Z",
     "iopub.status.busy": "2025-05-15T18:21:34.122607Z",
     "iopub.status.idle": "2025-05-15T18:21:41.501183Z",
     "shell.execute_reply": "2025-05-15T18:21:41.500622Z",
     "shell.execute_reply.started": "2025-05-15T18:21:34.122849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:21:41.503132Z",
     "iopub.status.busy": "2025-05-15T18:21:41.502448Z",
     "iopub.status.idle": "2025-05-15T18:21:53.871491Z",
     "shell.execute_reply": "2025-05-15T18:21:53.870833Z",
     "shell.execute_reply.started": "2025-05-15T18:21:41.503109Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f4c9e174c546bcb29a6af1501f19d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/65.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f50f8c85a54114b46ed269718225ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/223k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2318cc288fe4ce6980e93872106ee85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/91.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9469d28f914d3db212d6cfc94865b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/220k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534611b6babc4c64b05ab59116353cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9762bd35c3e1489db74aa14629884468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a4a2801eb8419e98ccc39659be6e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 500000\n",
      "Validation size: 2000\n",
      "Test size: 2000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"opus100\",\"en-it\")\n",
    "\n",
    "train_data = dataset[\"train\"].shuffle(seed=42).select(range(500_000))\n",
    "val_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_data))\n",
    "print(\"Validation size:\", len(val_data))\n",
    "print(\"Test size:\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:21:53.872337Z",
     "iopub.status.busy": "2025-05-15T18:21:53.872150Z",
     "iopub.status.idle": "2025-05-15T18:22:45.140426Z",
     "shell.execute_reply": "2025-05-15T18:22:45.139616Z",
     "shell.execute_reply.started": "2025-05-15T18:21:53.872321Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2294d1950c04ce09fff650dbe37bc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761765c182684fb28c99eb4c4077c264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5cfb8031784d8d9065a16a89de317e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-ZàèéìòùÀÈÉÌÒÙ\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def apply_cleaning(dataset_split):\n",
    "    return dataset_split.map(lambda x: {\n",
    "        \"translation\": {\n",
    "            \"en\": clean_text(x[\"translation\"][\"en\"]),\n",
    "            \"it\": clean_text(x[\"translation\"][\"it\"]),\n",
    "        }\n",
    "    })\n",
    "\n",
    "train_data = apply_cleaning(train_data)\n",
    "val_data = apply_cleaning(val_data)\n",
    "test_data = apply_cleaning(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:22:45.141974Z",
     "iopub.status.busy": "2025-05-15T18:22:45.141753Z",
     "iopub.status.idle": "2025-05-15T18:23:09.393305Z",
     "shell.execute_reply": "2025-05-15T18:23:09.392756Z",
     "shell.execute_reply.started": "2025-05-15T18:22:45.141958Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f627df5051b4467a59918a1c3516218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d969e1063a3476abd88282c9476d846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c248ba573047118c15e7db40a352ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_and_tokenize(example):\n",
    "    # Lowercase and whitespace-tokenize both source and target\n",
    "    src = example[\"translation\"][\"en\"].lower().strip().split()\n",
    "    tgt = example[\"translation\"][\"it\"].lower().strip().split()\n",
    "    return {\"src_tokens\": src, \"tgt_tokens\": tgt}\n",
    "\n",
    "# Apply to all splits\n",
    "train_data = train_data.map(clean_and_tokenize)\n",
    "val_data = val_data.map(clean_and_tokenize)\n",
    "test_data = test_data.map(clean_and_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:23:09.394125Z",
     "iopub.status.busy": "2025-05-15T18:23:09.393955Z",
     "iopub.status.idle": "2025-05-15T18:23:23.113607Z",
     "shell.execute_reply": "2025-05-15T18:23:23.112783Z",
     "shell.execute_reply.started": "2025-05-15T18:23:09.394110Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27e4c2c65e4485996cc1e3beb3f33dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d055e18ed3482ca968002ec573854f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de45ae944bf4420a0b3ea6a729cdb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_long_sentences(example, max_len=10):\n",
    "    return len(example[\"src_tokens\"]) <= max_len and len(example[\"tgt_tokens\"]) <= max_len\n",
    "train_data = train_data.filter(remove_long_sentences)\n",
    "val_data = val_data.filter(remove_long_sentences)\n",
    "test_data = test_data.filter(remove_long_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:23:23.114651Z",
     "iopub.status.busy": "2025-05-15T18:23:23.114441Z",
     "iopub.status.idle": "2025-05-15T18:23:32.307778Z",
     "shell.execute_reply": "2025-05-15T18:23:32.306949Z",
     "shell.execute_reply.started": "2025-05-15T18:23:23.114634Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(token_lists, max_vocab_size=12000):\n",
    "    counter = Counter()\n",
    "    for tokens in token_lists:\n",
    "        counter.update(tokens)\n",
    "    most_common = counter.most_common(max_vocab_size - 4)\n",
    "\n",
    "    vocab = {\n",
    "        \"<PAD>\": 0,\n",
    "        \"<SOS>\": 1,\n",
    "        \"<EOS>\": 2,\n",
    "        \"<UNK>\": 3\n",
    "    }\n",
    "\n",
    "    for idx, (token, _) in enumerate(most_common):\n",
    "        vocab[token] = idx + 4\n",
    "\n",
    "    return vocab\n",
    "\n",
    "# Build English (source) and Italian (target) vocabularies\n",
    "en_vocab = build_vocab(train_data[\"src_tokens\"], max_vocab_size=10000)\n",
    "it_vocab = build_vocab(train_data[\"tgt_tokens\"], max_vocab_size=10000)\n",
    "en_itos = {i: s for s, i in en_vocab.items()}\n",
    "it_itos = {i: s for s, i in it_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:23:32.308902Z",
     "iopub.status.busy": "2025-05-15T18:23:32.308644Z",
     "iopub.status.idle": "2025-05-15T18:24:28.698161Z",
     "shell.execute_reply": "2025-05-15T18:24:28.697397Z",
     "shell.execute_reply.started": "2025-05-15T18:23:32.308879Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a0799c3fe6454abb8e560537eb9c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/348175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569038c5e10546c1aca2c53077708da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8270bdf3a54a4982a77bf30197deb1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a8c146b6d74f699b39cdef1b6d8149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/348175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f55122fbaf6458d910932321a39c25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5552754f9132439ea3f4e15ade6db6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Train Size: 297,057\n",
      "Filtered Validation Size: 883\n",
      "Filtered Test Size: 863\n"
     ]
    }
   ],
   "source": [
    "\n",
    "src_counter = Counter(tok for sent in train_data[\"src_tokens\"] for tok in sent)\n",
    "tgt_counter = Counter(tok for sent in train_data[\"tgt_tokens\"] for tok in sent)\n",
    "\n",
    "\n",
    "def mark_rare_tokens(tokens, counter, threshold):\n",
    "    return [tok if counter[tok] >= threshold else \"<UNK>\" for tok in tokens]\n",
    "\n",
    "def replace_with_unk(example):\n",
    "    src = mark_rare_tokens(example[\"src_tokens\"], src_counter, threshold=2)\n",
    "    tgt = mark_rare_tokens(example[\"tgt_tokens\"], tgt_counter, threshold=2)\n",
    "    return {\"src_tokens\": src, \"tgt_tokens\": tgt}\n",
    "\n",
    "\n",
    "train_data = train_data.map(replace_with_unk)\n",
    "val_data = val_data.map(replace_with_unk)\n",
    "test_data = test_data.map(replace_with_unk)\n",
    "\n",
    "# Remove pairs containing <UNK>\n",
    "def remove_unk_pairs(example):\n",
    "    return \"<UNK>\" not in example[\"src_tokens\"] and \"<UNK>\" not in example[\"tgt_tokens\"]\n",
    "\n",
    "train_data = train_data.filter(remove_unk_pairs)\n",
    "val_data = val_data.filter(remove_unk_pairs)\n",
    "test_data = test_data.filter(remove_unk_pairs)\n",
    "\n",
    "# Show resulting dataset sizes\n",
    "print(f\"Filtered Train Size: {len(train_data):,}\")\n",
    "print(f\"Filtered Validation Size: {len(val_data):,}\")\n",
    "print(f\"Filtered Test Size: {len(test_data):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:24:28.699232Z",
     "iopub.status.busy": "2025-05-15T18:24:28.699009Z",
     "iopub.status.idle": "2025-05-15T18:24:28.703586Z",
     "shell.execute_reply": "2025-05-15T18:24:28.702915Z",
     "shell.execute_reply.started": "2025-05-15T18:24:28.699214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'Dont ever say that again', 'it': 'Non dirlo mai piu'}, 'src_tokens': ['dont', 'ever', 'say', 'that', 'again'], 'tgt_tokens': ['non', 'dirlo', 'mai', 'piu']}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:24:28.704674Z",
     "iopub.status.busy": "2025-05-15T18:24:28.704468Z",
     "iopub.status.idle": "2025-05-15T18:24:28.727284Z",
     "shell.execute_reply": "2025-05-15T18:24:28.726704Z",
     "shell.execute_reply.started": "2025-05-15T18:24:28.704658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_tokens_list, tgt_tokens_list, src_vocab, tgt_vocab, max_len=10):\n",
    "        self.src_tokens = src_tokens_list\n",
    "        self.tgt_tokens = tgt_tokens_list\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_seq = self.src_tokens[idx]\n",
    "        tgt_seq = self.tgt_tokens[idx]\n",
    "\n",
    "        src_ids = [self.src_vocab.get(token, self.src_vocab[\"<UNK>\"]) for token in src_seq]\n",
    "        tgt_ids = [self.tgt_vocab[\"<SOS>\"]] + \\\n",
    "                  [self.tgt_vocab.get(token, self.tgt_vocab[\"<UNK>\"]) for token in tgt_seq] + \\\n",
    "                  [self.tgt_vocab[\"<EOS>\"]]\n",
    "\n",
    "        # Pad or truncate\n",
    "        src_ids = self._pad_or_truncate(src_ids, self.src_vocab[\"<PAD>\"])\n",
    "        tgt_ids = self._pad_or_truncate(tgt_ids, self.tgt_vocab[\"<PAD>\"])\n",
    "\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "    def _pad_or_truncate(self, ids, pad_idx):\n",
    "        if len(ids) > self.max_len:\n",
    "            return ids[:self.max_len]\n",
    "        else:\n",
    "            return ids + [pad_idx] * (self.max_len - len(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:24:28.729587Z",
     "iopub.status.busy": "2025-05-15T18:24:28.729300Z",
     "iopub.status.idle": "2025-05-15T18:24:35.767931Z",
     "shell.execute_reply": "2025-05-15T18:24:35.767154Z",
     "shell.execute_reply.started": "2025-05-15T18:24:28.729563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create dataset instances\n",
    "train_dataset = TranslationDataset(\n",
    "    src_tokens_list=train_data[\"src_tokens\"],\n",
    "    tgt_tokens_list=train_data[\"tgt_tokens\"],\n",
    "    src_vocab=en_vocab,\n",
    "    tgt_vocab=it_vocab,\n",
    "    max_len=10\n",
    ")\n",
    "\n",
    "val_dataset = TranslationDataset(\n",
    "    src_tokens_list=val_data[\"src_tokens\"],\n",
    "    tgt_tokens_list=val_data[\"tgt_tokens\"],\n",
    "    src_vocab=en_vocab,\n",
    "    tgt_vocab=it_vocab,\n",
    "    max_len=10\n",
    ")\n",
    "\n",
    "test_dataset = TranslationDataset(\n",
    "    src_tokens_list=test_data[\"src_tokens\"],\n",
    "    tgt_tokens_list=test_data[\"tgt_tokens\"],\n",
    "    src_vocab=en_vocab,\n",
    "    tgt_vocab=it_vocab,\n",
    "    max_len=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:24:35.768976Z",
     "iopub.status.busy": "2025-05-15T18:24:35.768770Z",
     "iopub.status.idle": "2025-05-15T18:24:35.774008Z",
     "shell.execute_reply": "2025-05-15T18:24:35.773261Z",
     "shell.execute_reply.started": "2025-05-15T18:24:35.768959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, batch_size=64, shuffle=True):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = get_dataloader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = get_dataloader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = get_dataloader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:24:35.775248Z",
     "iopub.status.busy": "2025-05-15T18:24:35.774788Z",
     "iopub.status.idle": "2025-05-15T18:24:35.797574Z",
     "shell.execute_reply": "2025-05-15T18:24:35.796867Z",
     "shell.execute_reply.started": "2025-05-15T18:24:35.775229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqAttention(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_dim,\n",
    "                 src_pad_idx, tgt_sos_idx, tgt_eos_idx, n_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, embedding_dim, padding_idx=src_pad_idx)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, embedding_dim, padding_idx=src_pad_idx)\n",
    "\n",
    "        # Encoder: bidirectional LSTM\n",
    "        self.encoder_lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Decoder: unidirectional LSTM\n",
    "        self.decoder_lstm = nn.LSTM(\n",
    "            input_size=embedding_dim + hidden_dim * 2,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Attention: project decoder hidden → same size as encoder outputs\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc_out = nn.Linear(hidden_dim + hidden_dim * 2, tgt_vocab_size)\n",
    "\n",
    "        # Project encoder hidden/cell → decoder hidden/cell\n",
    "        self.bridge = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tgt_sos_idx = tgt_sos_idx\n",
    "        self.tgt_eos_idx = tgt_eos_idx\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = tgt.size()\n",
    "        vocab_size = self.fc_out.out_features\n",
    "\n",
    "        # ---- Encoder ----\n",
    "        embedded_src = self.dropout(self.encoder_embedding(src))  # [B, src_len, emb]\n",
    "        encoder_outputs, (hidden, cell) = self.encoder_lstm(embedded_src)  # encoder_outputs: [B, src_len, 2H]\n",
    "\n",
    "        hidden = self._combine_directions(hidden)  # [n_layers, B, 2H]\n",
    "        cell = self._combine_directions(cell)\n",
    "\n",
    "        hidden = torch.tanh(self.bridge(hidden))  # [n_layers, B, H]\n",
    "        cell = torch.tanh(self.bridge(cell))      # [n_layers, B, H]\n",
    "\n",
    "        # ---- Decoder ----\n",
    "        inputs = tgt[:, 0]  # <SOS>\n",
    "        outputs = torch.zeros(batch_size, tgt_len - 1, vocab_size).to(src.device)\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            input_emb = self.dropout(self.decoder_embedding(inputs)).unsqueeze(1)  # [B, 1, emb]\n",
    "\n",
    "            # Attention\n",
    "            attn_query = self.attn(hidden[-1]).unsqueeze(1)  # [B, 1, 2H]\n",
    "            attn_weights = torch.bmm(attn_query, encoder_outputs.transpose(1, 2))  # [B, 1, src_len]\n",
    "            attn_weights = F.softmax(attn_weights, dim=2)\n",
    "            context = torch.bmm(attn_weights, encoder_outputs)  # [B, 1, 2H]\n",
    "\n",
    "            # Decoder step\n",
    "            rnn_input = torch.cat((input_emb, context), dim=2)  # [B, 1, emb + 2H]\n",
    "            output, (hidden, cell) = self.decoder_lstm(rnn_input, (hidden, cell))  # output: [B, 1, H]\n",
    "\n",
    "            pred = self.fc_out(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))  # [B, vocab]\n",
    "            outputs[:, t - 1] = pred\n",
    "\n",
    "            # Next input\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = pred.argmax(1)\n",
    "            inputs = tgt[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs  # [B, tgt_len - 1, vocab]\n",
    "\n",
    "    def predict(self, src, max_len=50, device='cpu'):\n",
    "        self.eval()\n",
    "        src = src.unsqueeze(0).to(device)  # [1, src_len]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedded_src = self.encoder_embedding(src)  # [1, src_len, emb]\n",
    "            encoder_outputs, (hidden, cell) = self.encoder_lstm(embedded_src)\n",
    "\n",
    "            hidden = self._combine_directions(hidden)  # [n_layers, 1, 2H]\n",
    "            cell = self._combine_directions(cell)\n",
    "\n",
    "            hidden = torch.tanh(self.bridge(hidden))  # [n_layers, 1, H]\n",
    "            cell = torch.tanh(self.bridge(cell))      # [n_layers, 1, H]\n",
    "\n",
    "            inputs = torch.tensor([self.tgt_sos_idx], device=device)  # [1]\n",
    "            outputs = []\n",
    "\n",
    "            for _ in range(max_len):\n",
    "                input_emb = self.decoder_embedding(inputs).unsqueeze(1)  # [1, 1, emb]\n",
    "\n",
    "                attn_query = self.attn(hidden[-1]).unsqueeze(1)  # [1, 1, 2H]\n",
    "                attn_weights = torch.bmm(attn_query, encoder_outputs.transpose(1, 2))  # [1, 1, src_len]\n",
    "                attn_weights = F.softmax(attn_weights, dim=2)\n",
    "                context = torch.bmm(attn_weights, encoder_outputs)  # [1, 1, 2H]\n",
    "\n",
    "                rnn_input = torch.cat((input_emb, context), dim=2)  # [1, 1, emb + 2H]\n",
    "                output, (hidden, cell) = self.decoder_lstm(rnn_input, (hidden, cell))  # output: [1, 1, H]\n",
    "\n",
    "                pred = self.fc_out(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))  # [1, vocab]\n",
    "                top1 = pred.argmax(1).item()\n",
    "\n",
    "                if top1 == self.tgt_eos_idx:\n",
    "                    break\n",
    "                outputs.append(top1)\n",
    "                inputs = torch.tensor([top1], device=device)\n",
    "\n",
    "        return outputs  # List[int]\n",
    "\n",
    "    def _combine_directions(self, states):\n",
    "        # [2*n_layers, B, H] → [n_layers, B, 2H]\n",
    "        return torch.cat((states[0::2], states[1::2]), dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:24:35.798767Z",
     "iopub.status.busy": "2025-05-15T18:24:35.798288Z",
     "iopub.status.idle": "2025-05-15T18:24:36.437038Z",
     "shell.execute_reply": "2025-05-15T18:24:36.436499Z",
     "shell.execute_reply.started": "2025-05-15T18:24:35.798741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Seq2SeqAttention(\n",
    "    src_vocab_size=len(en_vocab),\n",
    "    tgt_vocab_size=len(it_vocab),\n",
    "    embedding_dim=256,\n",
    "    hidden_dim=512,\n",
    "    src_pad_idx=en_vocab[\"<PAD>\"],\n",
    "    tgt_sos_idx=it_vocab[\"<SOS>\"],\n",
    "    tgt_eos_idx=it_vocab[\"<EOS>\"],\n",
    "    n_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:24:36.437942Z",
     "iopub.status.busy": "2025-05-15T18:24:36.437699Z",
     "iopub.status.idle": "2025-05-15T18:24:40.614166Z",
     "shell.execute_reply": "2025-05-15T18:24:40.613648Z",
     "shell.execute_reply.started": "2025-05-15T18:24:36.437920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pad_idx = it_vocab[\"<PAD>\"]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Aliases for vocab access during translation/decoding\n",
    "src_vocab = en_vocab\n",
    "tgt_vocab = it_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:24:40.615590Z",
     "iopub.status.busy": "2025-05-15T18:24:40.615131Z",
     "iopub.status.idle": "2025-05-15T18:24:40.621031Z",
     "shell.execute_reply": "2025-05-15T18:24:40.620152Z",
     "shell.execute_reply.started": "2025-05-15T18:24:40.615561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, clip=1.0, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, tgt in tqdm(dataloader, desc=\"Training\"):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, tgt, teacher_forcing_ratio)  # [B, tgt_len-1, vocab]\n",
    "\n",
    "        output = output.contiguous().view(-1, output.shape[-1])  # [B*(tgt_len-1), vocab]\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)                  # [B*(tgt_len-1)]\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:24:40.622241Z",
     "iopub.status.busy": "2025-05-15T18:24:40.621988Z",
     "iopub.status.idle": "2025-05-15T18:24:40.676953Z",
     "shell.execute_reply": "2025-05-15T18:24:40.676284Z",
     "shell.execute_reply.started": "2025-05-15T18:24:40.622219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(dataloader, desc=\"Validation\"):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0.0)  # [B, tgt_len-1, vocab]\n",
    "\n",
    "            output = output.contiguous().view(-1, output.shape[-1])  # [B*(tgt_len-1), vocab]\n",
    "            tgt = tgt[:, 1:].contiguous().view(-1)                  # [B*(tgt_len-1)]\n",
    "\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:24:40.678007Z",
     "iopub.status.busy": "2025-05-15T18:24:40.677759Z",
     "iopub.status.idle": "2025-05-15T18:24:40.697473Z",
     "shell.execute_reply": "2025-05-15T18:24:40.696764Z",
     "shell.execute_reply.started": "2025-05-15T18:24:40.677984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_loop(model, train_loader, val_loader, optimizer, criterion, device,\n",
    "               n_epochs=10, clip=1.0, teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device,\n",
    "                                     clip=clip, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        val_loss = evaluate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        end_time = time.time()\n",
    "        mins, secs = divmod(int(end_time - start_time), 60)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1:02} | Time: {mins}m {secs}s\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val   Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(\"Saved best model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T18:24:40.698498Z",
     "iopub.status.busy": "2025-05-15T18:24:40.698208Z",
     "iopub.status.idle": "2025-05-15T19:06:50.523530Z",
     "shell.execute_reply": "2025-05-15T19:06:50.522410Z",
     "shell.execute_reply.started": "2025-05-15T18:24:40.698479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2321/2321 [04:14<00:00,  9.12it/s]\n",
      "Validation: 100%|██████████| 7/7 [00:00<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01 | Time: 4m 15s\n",
      "  Train Loss: 4.9452\n",
      "  Val   Loss: 4.6843\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2321/2321 [04:12<00:00,  9.18it/s]\n",
      "Validation: 100%|██████████| 7/7 [00:00<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02 | Time: 4m 13s\n",
      "  Train Loss: 4.1295\n",
      "  Val   Loss: 4.3157\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2321/2321 [04:11<00:00,  9.24it/s]\n",
      "Validation: 100%|██████████| 7/7 [00:00<00:00, 12.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 03 | Time: 4m 11s\n",
      "  Train Loss: 3.7791\n",
      "  Val   Loss: 4.1158\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2321/2321 [04:10<00:00,  9.25it/s]\n",
      "Validation: 100%|██████████| 7/7 [00:00<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 04 | Time: 4m 11s\n",
      "  Train Loss: 3.5443\n",
      "  Val   Loss: 3.9616\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2321/2321 [04:11<00:00,  9.21it/s]\n",
      "Validation: 100%|██████████| 7/7 [00:00<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 05 | Time: 4m 12s\n",
      "  Train Loss: 3.3719\n",
      "  Val   Loss: 3.9011\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2321/2321 [04:11<00:00,  9.21it/s]\n",
      "Validation: 100%|██████████| 7/7 [00:00<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 06 | Time: 4m 12s\n",
      "  Train Loss: 3.2350\n",
      "  Val   Loss: 3.8525\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2321/2321 [04:11<00:00,  9.24it/s]\n",
      "Validation: 100%|██████████| 7/7 [00:00<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 07 | Time: 4m 11s\n",
      "  Train Loss: 3.1256\n",
      "  Val   Loss: 3.7992\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2321/2321 [04:11<00:00,  9.23it/s]\n",
      "Validation: 100%|██████████| 7/7 [00:00<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 08 | Time: 4m 12s\n",
      "  Train Loss: 3.0350\n",
      "  Val   Loss: 3.7498\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2321/2321 [04:11<00:00,  9.22it/s]\n",
      "Validation: 100%|██████████| 7/7 [00:00<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 09 | Time: 4m 12s\n",
      "  Train Loss: 2.9486\n",
      "  Val   Loss: 3.7433\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2321/2321 [04:11<00:00,  9.22it/s]\n",
      "Validation: 100%|██████████| 7/7 [00:00<00:00, 11.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 | Time: 4m 12s\n",
      "  Train Loss: 2.8781\n",
      "  Val   Loss: 3.7002\n",
      "Saved best model.\n"
     ]
    }
   ],
   "source": [
    "train_loop(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    n_epochs=10,\n",
    "    clip=1.0,\n",
    "    teacher_forcing_ratio=0.4,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T19:07:45.743202Z",
     "iopub.status.busy": "2025-05-15T19:07:45.742651Z",
     "iopub.status.idle": "2025-05-15T19:07:45.750104Z",
     "shell.execute_reply": "2025-05-15T19:07:45.749506Z",
     "shell.execute_reply.started": "2025-05-15T19:07:45.743172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def compute_bleu(model, dataloader, tgt_vocab, idx_to_tgt, device, max_len=50, num_samples=100):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    sos_idx = tgt_vocab[\"<SOS>\"]\n",
    "    eos_idx = tgt_vocab[\"<EOS>\"]\n",
    "    pad_idx = tgt_vocab[\"<PAD>\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (src_batch, tgt_batch) in enumerate(dataloader):\n",
    "            if i * src_batch.size(0) > num_samples:\n",
    "                break\n",
    "\n",
    "            src_batch = src_batch.to(device)\n",
    "            tgt_batch = tgt_batch.to(device)\n",
    "\n",
    "            for src, tgt in zip(src_batch, tgt_batch):\n",
    "                pred_ids = model.predict(src, max_len=max_len, device=device)\n",
    "\n",
    "                ref = [idx_to_tgt.get(idx.item(), \"<UNK>\") \n",
    "                       for idx in tgt if idx.item() not in {pad_idx, sos_idx, eos_idx}]\n",
    "                hyp = [idx_to_tgt.get(idx, \"<UNK>\") \n",
    "                       for idx in pred_ids if idx != eos_idx]\n",
    "\n",
    "                references.append([ref])  \n",
    "                hypotheses.append(hyp)\n",
    "\n",
    "    bleu = corpus_bleu(references, hypotheses)\n",
    "    return bleu  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T19:07:49.014717Z",
     "iopub.status.busy": "2025-05-15T19:07:49.014175Z",
     "iopub.status.idle": "2025-05-15T19:07:49.019442Z",
     "shell.execute_reply": "2025-05-15T19:07:49.018844Z",
     "shell.execute_reply.started": "2025-05-15T19:07:49.014695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def translate_and_print(model, sentence, src_vocab, tgt_vocab, idx_to_tgt, device, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess and tokenize\n",
    "    tokens = sentence.lower().strip().split()\n",
    "    src_ids = [src_vocab.get(tok, src_vocab[\"<UNK>\"]) for tok in tokens]\n",
    "\n",
    "    # Convert to tensor and run prediction\n",
    "    src_tensor = torch.tensor(src_ids, dtype=torch.long).to(device)\n",
    "    pred_ids = model.predict(src_tensor, max_len=max_len, device=device)\n",
    "\n",
    "    # Decode predicted token ids\n",
    "    pred_tokens = [idx_to_tgt.get(idx, \"<UNK>\") for idx in pred_ids]\n",
    "\n",
    "    print(\"Source:      \", sentence)\n",
    "    print(\"Translation: \", \" \".join(pred_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T19:07:50.359137Z",
     "iopub.status.busy": "2025-05-15T19:07:50.358371Z",
     "iopub.status.idle": "2025-05-15T19:07:53.353040Z",
     "shell.execute_reply": "2025-05-15T19:07:53.352128Z",
     "shell.execute_reply.started": "2025-05-15T19:07:50.359108Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU Score (100 samples): 0.1146\n",
      "Source:       how are you\n",
      "Translation:  come sei come come\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx_to_tgt = {idx: tok for tok, idx in it_vocab.items()}\n",
    "\n",
    "bleu_score = compute_bleu(model, test_loader, it_vocab, idx_to_tgt, device, num_samples=500)\n",
    "print(f\"\\nBLEU Score (100 samples): {bleu_score:.4f}\")\n",
    "\n",
    "example_sentence = \"how are you\"\n",
    "translate_and_print(model, example_sentence, en_vocab, it_vocab, idx_to_tgt, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T19:09:55.854691Z",
     "iopub.status.busy": "2025-05-15T19:09:55.854404Z",
     "iopub.status.idle": "2025-05-15T19:09:55.891596Z",
     "shell.execute_reply": "2025-05-15T19:09:55.891026Z",
     "shell.execute_reply.started": "2025-05-15T19:09:55.854670Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:       good morning\n",
      "Translation:  buongiorno\n",
      "Source:       how are you\n",
      "Translation:  come sei come come\n",
      "Source:       do you have money\n",
      "Translation:  hai dei soldi\n",
      "Source:       I am very happy\n",
      "Translation:  sono molto felice felice\n",
      "Source:       nice to meet you \n",
      "Translation:  piacere di conoscerti\n",
      "Source:       the weather is nice\n",
      "Translation:  la <UNK> è è un\n",
      "Source:       i have been waiting for you\n",
      "Translation:  ho aspettato aspettando per te\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"good morning\",\n",
    "    \"how are you\",\n",
    "    \"do you have money\",\n",
    "    \"I am very happy\",\n",
    "    \"nice to meet you \",\n",
    "    \"the weather is nice\",\n",
    "    \"i have been waiting for you\"\n",
    "]\n",
    "for sentence in sentences:\n",
    "    translate_and_print(model,sentence,en_vocab,it_vocab,idx_to_tgt,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T19:07:58.540861Z",
     "iopub.status.busy": "2025-05-15T19:07:58.540578Z",
     "iopub.status.idle": "2025-05-15T19:07:58.866561Z",
     "shell.execute_reply": "2025-05-15T19:07:58.865651Z",
     "shell.execute_reply.started": "2025-05-15T19:07:58.540841Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of predicted <UNK>: 2.34%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "src_batch, tgt_batch = next(iter(train_loader))\n",
    "output = model(src_batch.to(device), tgt_batch.to(device), teacher_forcing_ratio=0.0)\n",
    "\n",
    "predictions = output.argmax(dim=-1)  \n",
    "unk_id = tgt_vocab[\"<UNK>\"]\n",
    "unk_ratio = (predictions == unk_id).float().mean().item()\n",
    "\n",
    "print(f\"Ratio of predicted <UNK>: {unk_ratio:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
